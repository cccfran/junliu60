  ,id,last,first,Remark,ins,time,title,abstract
0,1,Buja,Andreas,No,University of Pennsylvania,,,
1,2,Clyde,Merlise,No reply,Duke University,,,
2,3,Dawid,Philip,No,University of Cambridge,,,
3,4,Diebold,Frank,Yes,University of Pennsylvania,,On the Aggregation of Probability Assessments: Regularized Mixtures of Predictive Densities,"We propose methods for constructing regularized mixtures of density forecasts. We explore a variety of objectives and regularization penalties, and we use them in a substantive exploration of Eurozone inflation and real interest rate density forecasts. All individual inflation forecasters (even the ex post best forecaster) are outperformed by our regularized mixtures. From the Great Recession onward, the optimal regularization tends to move density forecasts’ probability mass from the centers to the tails, correcting for overconfidence."
4,5,Donoho,David,No reply,Stanford University,,,
5,6,Foster,Dean,Yes,Amazon,,Macau: A betting approach to decision making,
6,7,Johnstone,Iain,Not yet,Stanford University,,,
7,8,Reid,Nancy,No reply,University of Toronto,,,
8,9,Robert,Chris,Likely,University Paris-Dauphine,,The lasting impact of Tech Report No. 568,"In 1991, Charlie Geyer wrote a technical report that never got published, containing a proposal for approximating normalising constants and ratios of normalising constants via reverse logistic regression. While the notion sounds like inverse statistics, turning random variables into constants and vice versa in the regression, the method is consistent, highly efficient, relatively robust to the dimension of the integrand, and can relate to methods such as bridge sampling, noise contrastive estimation (NCE), and generative adversarial networks. We will highlight these connections and present recent results on an evaluation of the variability of the constant estimate, as well as the choice of the generative model. (This is joint work with Jean-Michel Marin and Judith Rousseau.)"
9,10,Rockova,Veronika,Yes,University of Chicago,,TSVS: Thompson Sampling for Variable Selection,"Thompson sampling is a heuristic algorithm for the multi-armed bandit problem which has a long tradition in machine learning. The algorithm has a Bayesian spirit in the sense that it selects arms based on posterior samples of reward probabilities of each arm. By forging a connection between combinatorial binary bandits and spike-and-slab variable selection, we propose a stochastic optimization approach to subset selection called Thompson Variable Selection (TVS). TVS is a framework for interpretable machine learning which does not rely on the underlying model to be linear. TVS brings together Bayesian reinforcement and machine learning in order to extend the reach of Bayesian subset selection to non-parametric models and large datasets with very many predictors and/or very many observations. Depending on the choice of a reward, TVS can be deployed in offline as well as online setups with streaming data batches. Tailoring multiplay bandits to variable selection, we provide regret bounds without necessarily assuming that the arm mean rewards be unrelated. We show a very strong empirical performance on both simulated and real data. Unlike deterministic optimization methods for spike-and-slab variable selection, the stochastic nature makes TVS less prone to local convergence and thereby more robust."
10,11,Rousseau,Judith,Yes,University of Oxford,,Using cut posterior for semi-parametric inference : the HMM case.," In this work we are interested in inference in Hidden Markov models with nite state space and nonparametric emission distributions. Since the seminal paper of Gassiat et al. (2016), it is known that in such models the transition matrix Q and the emission distributions F1, ... , FK are identifiable, up to label switching. We propose an (almost) Bayesian method to simultaneously estimate Q at the rate and the emission distributions at the usual nonparametric rates. To do so, we first consider a prior 1 on Q and F1,..., Fk which leads to a posterior marginal distribution on Q which verifies the Bernstein von Mises property and thus to an estimator of Q which is efficient. We then combine the marginal posterior on Q with an other posterior distribution on the emission distributions, following the cut-posterior approach, to obtain a posterior which also concentrates around the emission distributions at the minimax rates. In addition an important intermediate result of our work is an inversion inequality which allows to upper bound the L1 norms between the emission densities by the L1 norms between marginal densities of 3 consecutive observations."
11,12,Samworth,Richard,Yes,University of Cambridge,,"Entropy, ordering and shape constraints","I will discuss some new connections between maximum entropy distributions, orderings on distributions and shape-constrained density estimation methods. This is joint work with Rina Foygel Barber, Yining Chen and Ming Yuan."
12,13,Strawderman,Bill,Yes,Rutgers University,,On Minimax Shrinkage Estimation with Variable selectione Selection,"We study minimax estimators of the mean vector of a spherically symmetric distribution that also perform variable selection by estimating certain components as 0. The basic class of estimators developed is closely related to, and generalizes, classes considered by Zhou and Hwang [11] and Maruyama [8] in the Gaussian setting. The class  f distributions studied includes scale mixtures of normals (including Normal and Student-t) as well as the general class of spherically symmetric distributions with a residual vector. Certain subclasses of these estimators based on truncated order statistics are shown to be particularly effective when some information on the sparsity is known. Joint work with Stavros Zinonos, Rutgers University"
13,14,van der Vaart,Aad,Yes,Leiden University,,On Bayesian causal inference,We discuss some perspectives on estimating a causal effect using Bayesian methods.
14,15,Wasserman,Larry,Yes,Carnegie Mellon University,,Your Royal Highness,
15,16,Xu,Xinyi,Yes,Ohio State University,,From Minimax Mean Estimation to Minimax Density Prediction,"In a remarkable series of papers beginning in 1956, Charles Stein set the stage for minimax shrinkage estimators of a multivariate normal mean under quadratic loss. Almost 50 years later, parallel results were developed for predictive density estimation of a multivariate normal distribution under Kullback–Leibler loss.  In this talk, I review our journey of exploring the predictive density estimation problem, starting from the developments of new minimax shrinkage predictive density estimators to later extensions to admissible density estimators, normal linear regression estimators and nonparametric regression estimators."
16,17,McCulloch,Robert ,Yes,Arizona State University,,Multidimensional Monotonicity Discovery with MBART,"For the discovery of a regression relationship between y and x, a vector of p potential predictors, the flexible nonparametric nature of BART (Bayesian Additive Regression Trees) allows for a much richer set of possibilities than restrictive parametric approaches. To exploit the potential monotonicity of the predictors, we propose mBART, a constrained version of BART that incorporates monotonicity with a multivariate basis of monotone trees, thereby avoiding the confines of a full parametric form. Using mBART to estimate such effects yields (i) function estimates that are smoother and more interpretable, (ii) better out-of-sample predictive performance and (iii) less post-data uncertainty. By using mBART to simultaneously estimate both the increasing and the decreasing regions of a predictor, mBART opens up a new approach to the discovery and estimation of the decomposition of a function into its monotone components. "
17,17,De Veaux,Richard ,Yes,Williams College,,Ed Shrinks,I will highlight some of Ed's greatest achievements